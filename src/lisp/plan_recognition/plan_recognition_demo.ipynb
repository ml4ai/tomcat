{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan Recognition Demo\n",
    "## By Loren Champlin\n",
    "\n",
    "This is a demo of my plan recognition prototype algorithm. \n",
    "\n",
    "First, we assume that there is an agent who is trying to complete some objective (e.g, Search and Rescue) and that they are following a plan (i.e list of grounded actions) that accomplishes this objective. This plan is generated by some type of planner (e.g, Hierarchical Task Network), one that we engineered. We also assume that there are a number of different strategies the agent can adopt to complete their objective. These different strategies are represented as top-level tasks in the planner. Given that we observe the sequence of actions an agent is taking as well as the sequence of states that procedes each action (i.e, state, action pairs), the goal of our algorithm is to infer what top-level task was used to generate the agent's plan and thus infer what strategy they are using to complete their objective. \n",
    "\n",
    "The algorithm is implemented using a bayesian model. Let $S$ be a categorical random variable representing the possible strategies/top-level tasks and let $O_t = (s_0,a_0), (s_1,a_1), ... , (s_t, a_t)$ where $s_i$ and $a_i$ is the state and action at time $t$. Note that $s_i$ actually happens before $a_i$ (i.e, it is the state that precedes the action), but for simplicity we will assume we observe them at the same time. With these variables defined we can use Baye's Rule to compute,\n",
    "\n",
    "$P(S | O_t) = \\frac{P(O_t | S) P(S)}{P(O_t)}$\n",
    "\n",
    "Notice that $P(O_t)$ is just $\\sum{S}P(O_t | S) P(S)$, in other words we can compute $P(O_t | S) P(S)$ for each value of S and then just normalize the resulting probabilities to add up to 1 by dividing each one by $\\sum{S}P(O_t | S) P(S)$. \n",
    "\n",
    "The only challenge in this equation is computing the likelihood $P(O_t | S)$. However we can estimate the likelihood by assuming that our planner is complete (i.e, it can simulate the distribution of all possible plans the agent can execute). Therefore we can generate plans for each value of $S$ and see if $O_t$ matches any of the plan traces (i.e, plans with both states and actions, just like our observations) up to time $t$. This matching process allows us to compute, \n",
    "\n",
    "$P(O_t | S) \\approx \\frac{\\text{number of plans from $S$ that match $O_t$}}{\\text{total number of plans from $S$}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plan_recognition import upload_plans, posterior_dist, ob_dist\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ptt2trace(ptts):\n",
    "    traces = defaultdict(lambda:defaultdict(lambda:[]))\n",
    "    for i in ptts:\n",
    "        leaves = i.getleafNodes()\n",
    "        trace = []\n",
    "        for j in leaves:\n",
    "            trace.append(j.state)\n",
    "            trace.append(j.task)\n",
    "        traces[i.task][i.state].append(trace)\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next line uploads all possible plan traces from the Simple Schedule Domain. The domain has the agent executing a series of actions such as going to school, going to work, doing chores, etc. This schedule of tasks is determined by what day (Monday, Tuesday, Wednesday, Thursday, or Friday) it is. The day serves as the strategy or top-level task in this case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptts = upload_plans(\"simple-schedule-plan-traces.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = ptt2trace(ptts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (RAINING) (WORK-TODAY))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (RAINING) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (RAINING))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (RAINING))\n",
      "((FOUND-MOVIE) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY))\n",
      "((HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HUMAN ME) (RAINING) (WORK-TODAY))\n",
      "((HUMAN ME) (RAINING) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HUMAN ME) (NEED-GROCERIES) (RAINING))\n",
      "((HUMAN ME) (NEED-GROCERIES) (RAINING))\n",
      "((FOUND-MOVIE) (HUMAN ME) (RAINING))\n",
      "((HUMAN ME) (RAINING))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (WORK-TODAY))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (WORK-TODAY))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES))\n",
      "((HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES))\n",
      "((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME))\n",
      "((HAVE-HOMEWORK) (HUMAN ME))\n",
      "((FOUND-MOVIE) (HUMAN ME) (NEED-GROCERIES) (WORK-TODAY))\n",
      "((HUMAN ME) (NEED-GROCERIES) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HUMAN ME) (WORK-TODAY))\n",
      "((HUMAN ME) (WORK-TODAY))\n",
      "((FOUND-MOVIE) (HUMAN ME) (NEED-GROCERIES))\n",
      "((HUMAN ME) (NEED-GROCERIES))\n",
      "((FOUND-MOVIE) (HUMAN ME))\n",
      "((HUMAN ME))\n"
     ]
    }
   ],
   "source": [
    "for i in tr[\"MONDAY ME\"]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstration purposes, I grab one of the generated plans as the sequence of observed state, action pairs of the agent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0\n",
    "obs = []\n",
    "for i in ptts[p].getleafNodes():\n",
    "    obs.append((i.state,i.task))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we will assume that there is just one observation as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations \n",
      "\n",
      "True Top-level task:  MONDAY ME \n",
      "\n",
      "time:  0 , state:  ((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY)) , task:  !GO-TO-SCHOOL ME \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations \\n\")\n",
    "print(\"True Top-level task: \",ptts[p].task,\"\\n\")\n",
    "for i,j in enumerate(obs[:1]):\n",
    "    print(\"time: \",i,\", state: \",j[0], \", task: \",j[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the prior distribution of days is uniform (i.e, each day is equally as likely). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [{\"task\": \"MONDAY ME\", \"prior\": 1/5},{\"task\": \"TUESDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"WEDNESDAY ME\", \"prior\": 1/5},{\"task\": \"THURSDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"FRIDAY ME\", \"prior\": 1/5}]\n",
    "post = posterior_dist(obs[:1],cats,ptts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just one observation, we can see that all other days aside from Monday and Wednesday have been eliminated as possible top-level tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level task:  MONDAY ME , Posterior Belief:  0.5 \n",
      "\n",
      "Top-level task:  TUESDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  WEDNESDAY ME , Posterior Belief:  0.5 \n",
      "\n",
      "Top-level task:  THURSDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  FRIDAY ME , Posterior Belief:  0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in post:\n",
    "    print(\"Top-level task: \",i[\"task\"], \", Posterior Belief: \",i[\"posterior\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the same observation sequence with now two state, action pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations \n",
      "\n",
      "True Top-level task:  MONDAY ME \n",
      "\n",
      "time:  0 , state:  ((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WORK-TODAY)) , task:  !GO-TO-SCHOOL ME \n",
      "\n",
      "time:  1 , state:  ((FOUND-MOVIE) (HAVE-HOMEWORK) (HUMAN ME) (NEED-GROCERIES) (RAINING) (WENT-TO-SCHOOL) (WORK-TODAY)) , task:  !GO-TO-WORK ME \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations \\n\")\n",
    "print(\"True Top-level task: \",ptts[p].task,\"\\n\")\n",
    "for i,j in enumerate(obs[:2]):\n",
    "    print(\"time: \",i,\", state: \",j[0], \", task: \",j[1], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same prior was kept here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [{\"task\": \"MONDAY ME\", \"prior\": 1/5},{\"task\": \"TUESDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"WEDNESDAY ME\", \"prior\": 1/5},{\"task\": \"THURSDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"FRIDAY ME\", \"prior\": 1/5}]\n",
    "post = posterior_dist(obs[:2],cats,ptts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With just two observations, we can confirm that the top-level task is Monday. This of course might not be the case with a more complicated domain, which may take several observations to reach a posterior belief close to or at 1 for any specific top-level task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level task:  MONDAY ME , Posterior Belief:  1.0 \n",
      "\n",
      "Top-level task:  TUESDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  WEDNESDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  THURSDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  FRIDAY ME , Posterior Belief:  0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in post:\n",
    "    print(\"Top-level task: \",i[\"task\"], \", Posterior Belief: \",i[\"posterior\"], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the observation sequence is changed and we give the bayesian model the full sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 103\n",
    "obs = []\n",
    "for i in ptts[p].getleafNodes():\n",
    "    obs.append((i.state,i.task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations \n",
      "\n",
      "True Top-level task:  THURSDAY ME \n",
      "\n",
      "time:  0 , state:  ((HAVE-HOMEWORK) (HUMAN ME) (RAINING)) , task:  !GO-TO-SCHOOL ME \n",
      "\n",
      "time:  1 , state:  ((HAVE-HOMEWORK) (HUMAN ME) (RAINING) (WENT-TO-SCHOOL)) , task:  !DO-HOMEWORK ME \n",
      "\n",
      "time:  2 , state:  ((DID-HOMEWORK) (HUMAN ME) (RAINING) (WENT-TO-SCHOOL)) , task:  !DO-CHORES ME \n",
      "\n",
      "time:  3 , state:  ((DID-CHORES) (DID-HOMEWORK) (HUMAN ME) (RAINING) (WENT-TO-SCHOOL)) , task:  !PLAY-VIDEOGAMES ME \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Observations \\n\")\n",
    "print(\"True Top-level task: \",ptts[p].task, \"\\n\")\n",
    "for i,j in enumerate(obs[:4]):\n",
    "    print(\"time: \",i,\", state: \",j[0], \", task: \",j[1],\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [{\"task\": \"MONDAY ME\", \"prior\": 1/5},{\"task\": \"TUESDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"WEDNESDAY ME\", \"prior\": 1/5},{\"task\": \"THURSDAY ME\", \"prior\": 1/5},\n",
    "       {\"task\": \"FRIDAY ME\", \"prior\": 1/5}]\n",
    "post = posterior_dist(obs[:4],cats,ptts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice under the uniform prior, despite having the full observation sequence we don't overwhelming belief that the top-level task was Thursday (the true task) rather than Wednesday. It's obvious that both Thursday and Wednesday under this initial state produce the same plan and with any of the days being equally as likely in this case, we cannot rule out one over the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level task:  MONDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  TUESDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  WEDNESDAY ME , Posterior Belief:  0.5 \n",
      "\n",
      "Top-level task:  THURSDAY ME , Posterior Belief:  0.5 \n",
      "\n",
      "Top-level task:  FRIDAY ME , Posterior Belief:  0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in post:\n",
    "    print(\"Top-level task: \",i[\"task\"], \", Posterior Belief: \",i[\"posterior\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However it is possible that our prior belief could give us evidence to shift our posterior belief in favor of Thursday rather than Wednesday. This is shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = [{\"task\": \"MONDAY ME\", \"prior\": 3/20},{\"task\": \"TUESDAY ME\", \"prior\": 3/20},\n",
    "       {\"task\": \"WEDNESDAY ME\", \"prior\": 3/20},{\"task\": \"THURSDAY ME\", \"prior\": 2/5},\n",
    "       {\"task\": \"FRIDAY ME\", \"prior\": 3/20}]\n",
    "post = posterior_dist(obs[:4],cats,ptts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we are not getting a posterior belief of 1 like in other cases, we still achieved a majority posterior belief in the day being Thursday. Of ocourse a different prior could yield different results with Wednesday having the larger posterior belief instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-level task:  MONDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  TUESDAY ME , Posterior Belief:  0.0 \n",
      "\n",
      "Top-level task:  WEDNESDAY ME , Posterior Belief:  0.2727272727272727 \n",
      "\n",
      "Top-level task:  THURSDAY ME , Posterior Belief:  0.7272727272727273 \n",
      "\n",
      "Top-level task:  FRIDAY ME , Posterior Belief:  0.0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in post:\n",
    "    print(\"Top-level task: \",i[\"task\"], \", Posterior Belief: \",i[\"posterior\"],\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues to address\n",
    "\n",
    "There are two main issues to address with this prototype algorithm. \n",
    "\n",
    "The first issue is that the algorithm assume that the state of the domain is fully observable. For example, in the SAR missions we don't know where the victims are before hand. Therefore under a deterministic planner, we would not be able to generate actual plans. \n",
    "\n",
    "One fix is to have uncertainty in the planner (e.g, probablistic effects). In the case where there is uncertainty in the planner (i.e, the same initial state and top-level task can produce different plans), then we merely need to generate plans for each initial state and top-level task combo multiple times until we get a large sampling of the most likely plans. I suspect that this would cause only minor errors when estimating the likelihood of the bayesian model. \n",
    "\n",
    "Alternatively the planner could be deterministic and we could instead sample initial states (and assume those are fully observable states). In other words for a domain like the SAR mission, we would sample where the victims might be. To get a decent likelihood estimate in either case, we would need to somewhat accurately estimate certain distributions such as the initial state in the alternative case. \n",
    "\n",
    "Another fix for this issue is to generate a plan based off of a reduce but fully observable state reflecting the current state of for example the SAR mission. \n",
    "\n",
    "The second issue is that algorithm assumes an infallible execution of a plan generated by the corresponding planner. What this means in terms of the SAR mission is that the agent will never diverge from the strategy they using and will use the strategy perfectly. So if for example the player in the SAR mission has adopted the yellow-first strategy and for some reason they make just one action not charactistic of that strategy (e.g, triage a green victim in the dark bathroom before 5 minutes is up), then the plan recognition algorithm will rule out that strategy even if the player continues to follow that strategy for the rest of the game. \n",
    "\n",
    "One fix to this strategy is to have a moving window when doing the observation/plan matching. So instead of always matching the observations to possible plans from time 0, match them starting at some time i that increase with each new observation. This way a slight divergence from a strategy will only confuse our algorithm for a short time. \n",
    "\n",
    "Another fix is to assign a similarity measure between state, action pair sequences where a similarity of 1 means that the sequences are identical and a similarity of 0 means that they have nothing in common. Then we could estimate the bayesian model's likelihood by summing the similarity measures of each plan to the observed sequence. In other words,\n",
    "\n",
    "$P(O_t | S) \\approx \\frac{\\text{sum of similarity measures between plans from $S$ and $O_t$}}{\\text{total number of plans from $S$}}$\n",
    "\n",
    "I suspect that this would make it so that each value of S has at least some probability greater than 0 of generating any observation sequence. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
