#!/usr/bin/env python

"""Script for ASIST Study 3 evaluation"""

import json
from glob import glob
import logging
from logging import info, debug
import argparse
from tqdm import tqdm
import numpy as np
import pandas as pd
from datetime import datetime

logging.basicConfig(level=logging.WARN)


def run_evaluation(args):
    ac_use_counts = {}
    n_interventions = 0
    intervention_type_counts = {}
    output_file = open(args.output, "w")
    n_files_processed = 0

    df = pd.DataFrame(
        index=[
            "uaz_dialog_agent",
            "AC_IHMC_TA2_Player-Proximity",
            "AC_CMU_TA1_PyGLFoVAgent",
        ],
        columns=["n_interventions", "influence_level"],
    )
    df.n_interventions = np.zeros(len(df.index))
    df.n_interventions = df.n_interventions.astype(int)
    df.influence_level = df.influence_level.astype(str)

    for filepath in tqdm(glob(args.data_dir + "/*T00*UAZ*.metadata")):
        n_files_processed += 1
        info(f"Processing {filepath}")
        with open(filepath) as f:
            for line in f:
                message = json.loads(line)
                if (
                    "topic" in message
                    and message["topic"]
                    == "agent/intervention/ASI_UAZ_TA1_ToMCAT/chat"
                ):
                    n_interventions += 1
                    explanation = message["data"]["explanation"][
                        "info"
                    ].replace("This intervention was triggered", "")

                    intervention_type = ""

                    if "did not get an answer" in explanation:
                        intervention_type = "help_request_reply"
                        ACs_used = {
                            "uaz_dialog_agent",
                            "AC_IHMC_TA2_Player-Proximity",
                        }
                    elif "did not ask" in explanation:
                        if "critical victim" in explanation:
                            intervention_type = (
                                "help_request_for_critical_victim"
                            )
                            ACs_used = {
                                "uaz_dialog_agent",
                                "AC_CMU_TA1_PyGLFoVAgent",
                                "AC_IHMC_TA2_Player-Proximity",
                            }
                        elif "threat room" in explanation:
                            intervention_type = "help_request_for_room_escape"
                            ACs_used = {
                                "uaz_dialog_agent",
                                "AC_CMU_TA1_PyGLFoVAgent",
                                "AC_IHMC_TA2_Player-Proximity",
                            }
                        else:
                            raise (
                                ValueError(
                                    "Could not determine help request "
                                    "intervention type for explanation "
                                    f"'{explanation}'"
                                )
                            )

                    elif "placed a marker" in explanation:
                        intervention_type = "marker_block"
                        ACs_used = {
                            "uaz_dialog_agent",
                            "AC_IHMC_TA2_Player-Proximity",
                        }
                    elif "ensure" in explanation:
                        intervention_type = "motivational"
                        ACs_used = {}
                    else:
                        raise (
                            ValueError(
                                "Could not determine intervention type: ",
                                explanation,
                            )
                        )
                    intervention_type_counts[intervention_type] = (
                        intervention_type_counts.get(intervention_type, 0) + 1
                    )
                    debug(intervention_type + "|" + explanation)
                    for ac in ACs_used:
                        df.at[ac, "n_interventions"] = (
                            df.loc[ac].get("n_interventions", 0) + 1
                        )
                        # ac_use_counts[ac] = ac_use_counts.get(ac, 0) + 1

    influence_levels = [
        "Weak influence",
        "Moderate influence",
        "Strong influence",
    ]

    inds = np.digitize(
        df.n_interventions,
        bins=range(
            1, n_interventions, n_interventions // len(influence_levels)
        ),
    )
    for ac, ind in zip(df.index, inds):
        df.at[ac, "influence_level"] = influence_levels[ind - 1]


    def write(line):
        output_file.write(line + "\n")

    write(
        f"Study 3 evaluation report (generated {datetime.utcnow().isoformat()+'Z'})"
    )
    write("=================================================================")
    write("")
    write(f"Total number of files processed: {n_files_processed}")
    write("")
    write(f"Total number of interventions: {n_interventions}")
    write("")
    write("Intervention types")
    write("------------------")
    write(pd.Series(intervention_type_counts, name="Count").to_markdown())
    write("")
    write("AC Use Counts and Influence Levels")
    write("----------------------------------")
    write(
        df.to_markdown()
    )

    output_file.close()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument(
        "--data_dir",
        help="Directory containing .metadata files",
        default="/media/mule/projects/tomcat/protected/study-3_2022",
    )
    parser.add_argument(
        "--output",
        help="Path to output report file",
        default="study_3_evaluation.md",
    )

    args = parser.parse_args()
    run_evaluation(args)
