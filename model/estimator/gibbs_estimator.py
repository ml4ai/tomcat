from sampling.ancestral_sampling import AncestralSampling
from sampling.gibbs_sampling import GibbsSampling
import copy
import pandas as pd
from tqdm import tqdm


class GibbsEstimator(GibbsSampling):

    def __init__(self, pgm):
        self.pgm = copy.deepcopy(pgm)

    def estimate_parameters_from(self, pgm, data, burn_in_periods, number_of_samples):
        parameter_samples = self.sample_parameters(data, number_of_samples, burn_in_periods)
        parameter_estimation = parameter_samples.mean()
        parameter_estimation.index = pd.MultiIndex.from_tuples(parameter_estimation.index)
        parameter_estimation = parameter_estimation.droplevel(1)

        return parameter_estimation

    def sample_parameters(self, data, number_of_samples=500, burn_in_periods=100):
        if data.empty:
            raise TypeError('Data is mandatory for parameter estimation')

        samples = []

        # The following procedure will complete the data with samples for
        # the latent variables using ancestral sampling
        data_and_samples = self.get_initial_estimates(data)

        # Parameter nodes are constant which means their value are the same in the samples
        # generated by the above procedure. We need to assign their initial values to their
        # respective nodes in the graph so other nodes that have distribution depending on
        # these parameters can get their current sampled value from there
        sample = data_and_samples.iloc[0]
        sample = sample[
            self.pgm.get_parameter_nodes_id()]  # Sample contains only values assigned to the parameter nodes
        self.assign_values_to_nodes(sample)

        # Nodes we have to iterate over in the gibbs sampling, that is, the ones not observed in data
        latent_nodes = [node for node in self.pgm.get_nodes() if node.get_id() not in data.columns]

        for _ in tqdm(range(burn_in_periods), desc="Burn-in"):
            sample, data_and_samples = self.get_single_sample_estimate(latent_nodes, data_and_samples, sample)

        for _ in tqdm(range(number_of_samples), desc="Samples"):
            sample, data_and_samples = self.get_single_sample_estimate(latent_nodes, data_and_samples, sample)
            samples.append(sample)

        return pd.DataFrame(samples)

    def get_single_sample_estimate(self, latent_nodes, data_and_samples, last_sample):
        for latent_node in latent_nodes:
            if latent_node.metadata.constant:
                # Constant nodes are out of the nodes in the data plate. They only have one sample
                # per all data points
                posterior = self.get_posterior(latent_node, data_and_samples)
                sampled_value = posterior.sample()
                assignment = pd.Series({0: sampled_value}).repeat(len(data_and_samples.index)).reset_index(drop=True)

                if latent_node.metadata.parameter:
                    last_sample[latent_node.get_id()] = sampled_value
                    latent_node.assignment = sampled_value
            else:
                assignments = []
                markov_blanket_ids = list(self.pgm.get_markov_blanket_ids(latent_node))
                posterior_by_mb_assignments = {}
                for _, data_point in data_and_samples.iterrows():
                    # Uniquely identify a the MB nodes and their assignment as a string
                    markov_blanket_assignments_key = '{}'.format(data_point[markov_blanket_ids].to_dict())
                    if markov_blanket_assignments_key in posterior_by_mb_assignments:
                        posterior = posterior_by_mb_assignments[markov_blanket_assignments_key]
                    else:
                        posterior = self.get_posterior(latent_node, pd.DataFrame(data_point).transpose())
                        posterior_by_mb_assignments[markov_blanket_assignments_key] = posterior

                    assignments.append(posterior.sample())

                assignment = pd.Series(assignments)

            # Update the columns of the sampled values for this latent node with the samples
            # we got for each data point
            data_and_samples[latent_node.get_id()] = assignment

        return last_sample, data_and_samples

    def get_initial_estimates(self, data):
        """
        This function completes the data with sampled values for each latent variable in each data point.
        Constant nodes will preserve the value sampled in the first data point
        """
        sampling = AncestralSampling(self.pgm)
        samples = []

        copied_data = data.copy()
        number_of_data_points = len(data.index)

        # Constant nodes will have a constant value for all the sampled data
        sample = sampling.sample()
        for constant_node in self.pgm.get_constant_nodes():
            copied_data[constant_node.get_id()] = sample[constant_node.get_id()].repeat(
                number_of_data_points).reset_index(drop=True)

        for _, data_point_observations in copied_data.iterrows():
            sample = sampling.sample(observations=data_point_observations)
            samples.append(sample)

        return pd.concat(samples, ignore_index=True)
