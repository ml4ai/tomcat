\section{Task 2: Closed loop communication (CLC) detector}
\subsection{Introduction}

Closed-loop communication (CLC) is often recommended in the team research
literature as a communication behavior that can guarantee the accuracy of
information exchange. In this project, we developed a rule-based natural
language process (NLP) algorithm for detecting the presence or absence of CLC
procedures in spoken dialogue within teams of humans collaborating on shared
tasks.

The ASR Agent in conjunction with the DialogAgent developed by the ToMCAT team
enables us to extract events of interest in real-time from spoken dialogue,
which goes a long way towards addressing both of these limitations. However,
the DialogAgent at the level of individual utterances, and does not reason
about multi-utterance context. For this reason, we are developing a separate
downstream CLC detection component that utilizes the outputs of the
DialogAgent, but also reasons about context and state more deeply.

\subsection{Approach}

There are three distinct phases in a complete Closed-loop communication:

\begin{enumerate}
    \item Call-out: The sender initiates a message.
    \item Check-back: The receiver acknowledges the message, usually by paraphrasing or repeating the main information of the message.
    \item Closed-loop: The sender verifies that the message has been received and interpreted correctly.
\end{enumerate}

Our rule-based EE system uses the Odin (Valenzuela-Escárcega et al., 2016)
event extraction framework, and currently contains 420 active rules that can
extract from simple to complex events, as well as entities and sentiment. Based
on these event rules, we develop an algorithm to detect the three phases of
CLC. First, the Call-out phase will be detected by rules in the DialogAgent.
Labels including HelpRequest, Instruction, and NeedAction are used as triggers
for this event. Next, we examine a window of the subsequent five utterances in
the dialogue.  If we find Move or Commitment labels by other team members as
well as the same semantic labels as in the Call-out message, then this
indicates there is a Check-back phase. However, we also care about the weak
Check-back situations where the team member only acknowledged the Call-out but
didn’t repeat the main information of the Call-out message. We assign the weak
version of Check-back a 0.5 score, compare to the full score of 1.  Finally,
within five utterances window, if we see the sender verified the information of
the Check-back with the Agreement label, we can determine that this is a
closed-loop communication. The final score of the Closed-loop communication
detected is the sum of the scores of the three phases.

\subsection{Evaluation}

To evaluate the algorithm of CLC detection, we conducted a CLC annotation task
for 3239 of the dialogue utterances from our ASIST Study-3 transcriptions, and
the rule-based CLC extractions and scores are tested on the golden annotation
regarding the precision, recall, and F1.


\subsection{Results}

Our CLC detection system extracted 978 closed-loop communication patterns,
scoring from 1.0, for which only the Call-out is detected but no Check-backs
and Closed-loop, to 4.0, for which two team members had checked back to one
call-out, and the initiator closed the loop at the end. However, our annotators
only detected 155 closed-loop communication patterns, indicating that the
rule-based CLC detection system is suffering from overgeneralizing issues at
this stage. A t-test between the CLC scores marked by the annotators and the
CLC scores assigned by our detection system shows that the CLC detection system
doesn’t give significantly higher scores for CLC detection than the annotators
(p > 1). 

Table 1 shows the results of our evaluation. Because the participants in our
game tasks are not specifically trained to use Closed-loop communication, there
is a relative rear distribution of the CLC pattern. We can see from the table
that although the overall F1 is as high as 0.79, the score is largely boosted
by the NA labels. Those are the utterances that don’t recognize by the
annotators and our system as a part of the closed-loop communication.  However,
for the individual phases of the CLC, we do see higher recall scores than
precision. This indicates that our rule-based CLC detection system can pick as
many of the phases as it can, but it also included too much noise so that it’s
not precise. 

\subsection{Error Analysis}

This result is caused by three major reasons: Firstly, for the Call-out phase,
there’s a competing nature between precision and recall. For example, the
Call-out is usually a question initiated by the initiator, but there are also a
fair amount of questions that don’t necessarily initiate a CLC session, like
``shall we go'', ``where are you''.  Secondly, when we look for recursive labels
between Call-out and potential Check-backs, overgeneralization could easily
happen to introduce noise. A lot of the time, the recursive labels are just
very common labels like Room or Victim, but the speakers are actually talking
about different things. Finally, the overmatching of the Close-loop phase is
generally caused by only looking for a simple acknowledgment or a Gratitude
label. But they are also very common labels that will easily cause
overmatching.



Precision
Recall
F1
Support
NA
0.985
0.722
0.833
3084
Call-out
0.123
0.774
0.212
155
Check-back
0.118
0.721
0.203
147
Close-loop
0.021
0.550
0.041
20
Overall
0.949
0.702
0.798
3239


Table 1  Evaluation of the rule-based CLC detection system

\subsection{Conclusion}

As a creative method to break the limit of single utterances and building a
multi-speaker, multi-event extraction system in the context, this project made
an attempt to prove the potential of it. However, as the result is not ideal
for now, we see many areas that we should work on to adjust the current system.
For further development of the system, we’re also planning to introduce the
machine learning way to deal with the issues that rule-based system has issues
on. 
