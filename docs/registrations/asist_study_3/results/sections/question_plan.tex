\chapter{Question-Asking and Plan Inference}
\label{ch:question_plan}
\textbf{Salena Ashton, Stephen Kim, Loren Rieffer-Champlin, Liang Zhang,
Adarsh Pyarelal, Clayton Morrison}

\section{Introduction}

In the SAR scenario for ASIST's Minecraft virtual environment, teams of human players engage in cooperative behavior to search for, stabilize, and rescue victims within a collapsed building. As human players verbalize plans, make suggestions, or tell each other what to do, they also ask questions that can infer hidden goals or intentions. Teammates reduce their individual knowledge asymmetry by asking and answering questions. Using Theory of Mind (ToM), \footnote{The capacity to infer another's thoughts, feelings, beliefs or intentions.}, we will investigate how uttered questions can infer another person's goal or intention. 

This investigation will guide future research into knowledge engineering and representation of tasks, goals, and hidden intentions. While simple human goals may be represented with classical AI planning, complex goals that have constraints or multiple levels of abstraction may be best represented by a \emph{hierarchical task network} (HTN), which is a tree of possible plans.\footnote{Technically, the plans
    produced by HTN planners can also be represented with flat lists - however,
in this section, we use the term `plan' to refer to the actual `plan tree' that
contains the task decompositions as well, rather than just the plan alone.}




We will investigate the following research questions for Study-3:

\begin{enumerate}
    \item How do spoken questions reveal another person’s plan or intent? 
    \item Can people listen to spoken questions and accurately decide if the other person's plan
        is simple, sequential, or hierarchical? Can the distinction between such
        structures improve predictive performance for Artificial Social
        Intelligence (ASI) agent intervention?
\end{enumerate}

%%%%%
Previous research into question-\emph{answering, not asking,} centered on optimization because researchers assumed that people prefer concise questions and answers. However, people do not engage in dialog using question or answer sets. They may ask open-ended questions, meander in speech without purpose, and use indirect speech acts to express mutual goals or build rapport with each other. The current literature regarding ToM and question-answering is sparse but even more so for question \emph{asking}. 

Hawkins and Goodman connect question-asking and intention \emph{because} of the scarcity of “empirical evidence about how social context affects the questioner’s choice." They redefined the meaning of a question as "the interpretation and response of a hidden (or uttered) goal, to be discerned by another person, typically a dialog partner" \citep{hawkins_goodman_2017}. Hawkins and Goodman describe speech acts and question-answer dialog as a form of \emph{information asymmetry}: A questioner has a goal but needs information while an answerer has information but does not know the questioner’s goal. The type of questions then asked will depend on context, social inference, and signals in a dialog setting. The significance of their work is in the decoupling of the inferred goal from the explicit meaning of the question to model context and avoid assumptions. \footnote{Their work was limited to epistemic questions and cooperative behavior.}

Deciphering a person’s goal or intention from their answer to a question, instead of the question itself, may be another way to understand intent. While Hawkins and Goodman define questions as hidden goals or intentions, Mehdi Alaima et al define the act of asking questions as ‘the providing of information or knowledge to reinforce knowledge one way or another," independently paralleling the definition given by Hawkins and Goodman. "When information is missing, or contradicts what one knows, a knowledge goal will arise, often leading to the generation of questions. The person is then made aware of the information needs, and motivated to formulate a question to obtain the missing knowledge,” \citep{alaimi_2020}. Building on the claims of Hawkins and Goodman, and Alaima et al, we investigate question-asking within the SAR scenario of ASIST’s Minecraft environment. 


\section{Approach}

We assume that questions have hidden goals and infer plans. As teams ask more
questions of each other, human team ToM converges toward cooperative behavior.
We will investigate whether question-asking is associated with \emph{team
planning}, defined as a set of goals, strategies, or tasks that are executed.
We define \emph{coordination} as behaviors and utterances to create a common
plan or strategy\footnote{Note that this is distinct from the mathematical
definition of coordination proposed in \autoref{ch:pgm}}. We define
\emph{cooperation} as team behaviors that implement an already-agreed up on
plan.

To capture hidden goals, inferred plans, and patterns that may represent human
ToM, we will annotate six ASIST Study 3 Spiral 2 pilot video observations and
six HSR ASIST Study 3 videos released between March 29 and May 5, for a total
of at least twelve videos. These videos are of three distinct missions for each
team. Due to the expensive costs of taxonomy label development with strict
adherence to grounded theory methodology, this stage of the experiment is
limited to no less than twelve videos. 

Two human annotators will code all uttered questions between teammates within the Minecraft SAR scenario.
We use the qualitative coding procedure known as Grounded Theory,
as defined by \citet{corbin_strauss_2015}. 

More specifically, and as defined by \citet{saldana_2021}, we will use a
Grounded Theory Process Coding for a state or action across some interval of
time. These \emph{grounded-in-data} labels are known as \emph{concept-level}
labels, which are the smallest pieces of data that encode a question-asking
phenomena of interest.

More specifically, and as defined by \citet{saldana_2021}, we will use a
Grounded Theory Process Coding for a state or action across some interval of
time. These \emph{grounded-in-data} labels are known as \emph{concept-level}
labels, which are the smallest pieces of data that encode a question-asking
phenomena of interest. We also use this coding methodology to investigate the connectivity and
causality of each concept label to discover possible relationships between
presence or absence of team actions, interactions, conditions, and consequences
of question-asking. Densely-connected concept labels suggest subcategories and
categories. Sparsely-connected labels will not be discarded; they will be used
to consider variability within patterns and categories that emerge. In cases
where questions have co-reference or other contextual dependencies, only that
direct dependency will be coded for local semantic meaning.

We make the following considerations when creating codes: 

\begin{itemize}
    \item Frequency will not dictate importance, causality, or connectivity of a concept
    \item Each question will have at least one annotation and up to four
      annotations:
    \begin{itemize}
        \item Primitive actions (ground truth). Ex: breakRubble,
          requestStabilizedVictimCarry.
        \item Abstraction Levels of actions (of primitive actions) Respective
          examples: respondRubbleRequest or createVictimAccess, collaborateStabilizedVictim
    \end{itemize}
    \item Labels will be stemmed and minimally normalized
    \item Capturing the phenomena of question-asking across time, between any subset of a team, between the same team across the two different missions. 
\end{itemize}


To avoid annotator and researcher biases and any \emph{a priori} belief on
which team ToM strategies may be used, concept and category labels are not
pre-determined. Inter-annotator agreement must reach a Kappa Score of 80\% or
higher. This also gives a more solid, grounded analytical meaning to any
emergent categories. 

After the development of labels and taxonomy, the investigation of team ToM and
question-asking will scale for additional videos. When all concepts,
subcategories, categories can reasonably explain the phenomena of the video
observations, one or two super-categories, \emph{theories of team plan}, will
emerge. We currently assume that a theory of team plan would have greater
predictive power and ToM inference potential. 


\section{Evaluation}

Because of the small sample size of this investigation, we will not perform a
quantitative evaluation at this time. Instead, we will perform a qualitative
investigation of word frequencies, clustering patterns, and correlation of
annotator-generated labels through data visualization. Below is a list of
possible visualizations we may consider:

\begin{itemize}

    \item Connectivity of concept-level labels: radial diagrams, arc diagrams,
        matrix diagrams or graph networks

    \item Frequency patterns of words or concept labels (normalized, word count
        / total number of words in that question): scatterplots or histograms

    \item Correlation of words and labels with time: time series, scatterplots. 

    \item Concept-level subcategorization(s): clustering, PCA (concept labels
        possibly projected onto sub-categorical spaces), or hierarchical
        visualizations.

\end{itemize}


Such visualizations, based on twelve videos, will lead to further insight
through this investigation. Future measures may include Mann-Whitney U-Tests,
t-tests (only if we annotate a large-enough sample), precision and recall of
the concept-level and category patterns to describe the generalizability for
real data with no ASI interventions, generalizability for real data with ASI
interventions, and the variance of patterns in label categories. Another
possible measure, for future research, would be the F1 score to explain how
well these labels describe observations without ASI interventions, when
compared to high-intervention observations. This future investigation would
address measure ASI-M5: Coordinative Communications to
measure teamwork, include additional video observations for real data in Study
3, and continue our investigation of whether human plans and ToM are best
represented by classical planning or HTN planning.



\section{Results}

\subsection{Label Development and Taxonomy}
We investigated the connection between question-asking and hidden goals of the question-asker with strict qualitative coding approach known as Grounded Theory, ensuring that our annotations were not biased by prior research, pre-determined labels, or theories about player strategy and ToM. This data-driven, robust method captures player intention, team strategy, and naturally lends itself to quantitative analysis. 

Two independent annotators\footnote{Ashton and Kim} viewed six study-3 pilot videos with unconstrained, unsupervised labels. Videos were selected from no-human intervention videos, at random, and three teams were chosen as a sequence (team 217, 218 and 219). The continuous disambiguation of unsupervised labels, using lexical definitions, led to extensive documentation of which labels are to be used for the capturing of specific phenomena in observations.\footnote{For example, when an annotator should use the verb 'ask', 'clarify', or 'request'.} The lexical development of labels was used for the annotation of six different study-3 real data videos\footnote{Ashton and Reiffer-Champlin}, which then lead to Theoretical Saturation \footnote{Theoretical saturation is the point when few additional labels are created, regardless of fully-autonomous annotators' ability to create new ones. Saturation demonstrates that the existing label schema fully capture the events and phenomena observed.} and converged around 21 verb labels, 24 noun labels, 25 modifier labels. Without replacement, this yields 12,600 possible label combinations, yet Ashton demonstrated theoretical saturation through the independent construction of less than 100 labels from two autonomous annotators.


\subsection{Data Cleaning and Annotator Agreement}
We achieved an unweighted Cohen Kappa\footnote{In order to adhere to the robust procedure of Grounded Theory, the technical calculation of Kappa's score would include the probability of \textit{all possible probabilities of all labels}, which would approach zero. While it sounds trivial, it must be mentioned that the reason Grounded Theory is robust in its unsupervised labeling approach is because annotators have complete autonomy. Because the sample size is small and only two annotators worked on the real data, there is no current justification for using Scott's pi or a weighted Kappa. This will change as investigation and further research scale.} agreement of 0.892. Using the disambiguation documentation and Merriam-Webster's Dictionary to meticulously settle any annotator label disagreement, Ashton declared the question labels as 'agree' or 'disagree'. When annotator intention and the disambiguation documentation did not clarify the agreement or disagreement of labels, Ashton declared it as 'disagreement'. 



\subsection{Emergent Categories, Team Theories, and Preliminary Patterns}

About 100 labels that resulted from real data annotations were analyzed for emergent categories and preliminary patterns. Frequency of labels \emph{did not} dictate importance, data were minimally lemmatized and label concept granularities were resolved using Bayes' Theorem.\footnote{For example, the conditional probabilities of questions given a player intention,  and the probability of intention, given a question, were calculated. Then possible granularity of labels, such as replacing "critical" and "regular victim" with one label, "victim" were considered . If the probabilities changed with this replacement, the granularity of the two original labels remained. Global placement after post processing is not reflected in the Kappa score in order to maintain raw data and researcher integrity.} Data visualization and preliminary analysis focus on three key features of the data: 'questionLabel'\footnote{Code representation of the question utterance and no further context except for co-reference resolution. Some questions were spoken as statements with inflection. Other questions were actually demands shrouded with politeness. All three types of questions were coded for this investigation.}, 'abstractLabel'\footnote{Code representation of player intention. This was captured by taking the question into context. For example, if the question were represented by 'requestBreakRubble', the intent behind a request could have been 'collaborateCriticalWake' or 'navigateLocation'. AbstractLabel is not the representation of \emph{why} a question had been asked; it is the \emph{intention} of the player who asked the question.}, and 'utterance', the word of interest uttered in a question. From the emergent categories, we chose specific words that optimized intention or belief among player interaction\footnote{For example: \emph{ask}, \emph{request}, \emph{suggest}, and \emph{clarify} are all actions that show the goal of obtaining information, but clarify asks for additional information needed before acting upon a goal or task, request is an initiation of commitment to another player with the optional accept/ reject response that suggest would not imply, and ask is a general desire for information.}. 

The following themes emerge from the data:
\begin{itemize}
    \item Less Team Collaboration (talking \emph{at or to} a teammate and not \emph{with} a teammate): direct, suggest, etc.
    \item More Team Collaboration (talking \emph{with} a teammate): ask, request, answer, clarify, etc.
    \item Intention toward position: location or destination, but not necessarily navigation strategies.
    \item Prioritizing an idea: plan, suggest, request, collaborate, critical, or any victim.
    \item Question utterances that were actually demands, statements that were questions with inflection, and other nuanced utterances: suggest, tell, direct, request; context and explanations included in annotation.
\end{itemize}


\subsection{Discussion and Future Research}

The ultimate goal of this registration is two-fold: find enough evidence to warrant the continued investigation of question-asking through causal reasoning to discern player intention and to algorithmitize this intent from utterance transcriptions of player without human annotator dependency. Due to the small sample size, we do not offer conclusive evaluations at this time. However, from these 12 video annotations, we \emph{warrant} the further investigation of question-asking and inferred goals among human team players.
\vspace{15pt}
Future research will include the following tasks:
\begin{itemize}
    \item Scale for additional data: Ashton is currently building a model that maps words\footnote{Bag of Words and other models, at word and sentence level.} to intentions.
    \item Natural generation model: Use the above model to re-create the labels generated by Ashton and Champlin. We will compare these results to models developed by the University of Arizona ToMCAT's NLP work group, including the use of ODIN and ASR.
    \item Additional human annotation of data to compare models in earlier stages until the automation of linking intention with natural language is credible and reproducible.
    \item Compare and contrast emerging team ToM from no-intervention observations to high-intervention observations with quantitative analyses and causal inference models.
\end{itemize}

It is through these models and analyses that we will investigate the intent of players who ask questions. Additional applications that can improve predictive performance for Artificial Social Intelligent agent intervention.


\section{Figures and Additional Preliminary Analyses}
%%%%%%%%%%%%%%% Visuals  %%%%%%%%%%%%
%% Joint Probabilities

\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/questionLabelProbability_STA.pdf}}
    \caption{Player question utterances, represented by question labels and displayed by each team and the most probable question labels.}
\end{figure}

Figure 1 shows the highest probability of questions to be asked: requests for teammates to come, clarification of a victim's status for transport, shown by the word 'stabilized' and not by 'victim' (an unstabilized victim), and clarification of any victim's location. Notice that team 219, who scored the highest of the three, asked more questions to clarify communication with each other. Previous assumptions about players who prioritized the clearing of rubble, navigation strategy or dividing up tasks is not supported in this preliminary investigation.


\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/abstractLabelProbability_STA.pdf}}
    \caption{Player intention, represented by abstract labels and displayed by each team and the most probable abstract label. }
\end{figure}

Figure 2 shows the highest probability of intentions, expressed by questions, visual cues, context, and player interaction (represented as an abstract label). These include collaborating with teammates about waking critical victims and carrying stabilized victims to the proper sick bay. Team 219, who scored some of the highest combined scores in many of the observations, not just the ones annotated, compared to 218, who scored some of the lowest in the data, intended to collaborate on carrying stabilized victims to the proper sick bay. The difference in score is found in more granular actions: while team 219 often requested for collaborative tasks, team 218 would more often demand or tell others what tasks to perform (as seen in figure 1).


\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/questionLabel_ConditionalProbability_STA.pdf}}
    \caption{Conditional probability of questions asked, given the player's intention.}
\end{figure}

Figure 3 shows the variation of questions that can be asked, given player intention. More than 40\% of questions that could be asked are marked 'other.'\footnote{Abstracted into 'other' category for clarity of this visual.} It is this variation that matters: players will not necessarily ask expected questions (or any question at all) just because they have intention. The most likely questions that \emph{are vocalized} have one aspect in common: they all have verbs that are collaborative in nature: request, clarify, collaborate. Verbs that are less collaborative in nature, (direct, suggest, inform) do not show up as the most common questions. This suggests that when players feel like they are part of the team, they are more likely to ask questions that have a give-and-take dialog.


\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/abstractLabel_ConditionalProbability_STA.pdf}}
    \caption{ Conditional probability of player intention, given the question that is asked.}
\end{figure}

Figure 4 shows that a question to clarify the stabilized victim's status has a theoretical probability\footnote{It is not justified to claim 100\% probability.} approaching 100\% intent to collaborate with others on where to deliver that victim. Players were most likely to ask for unique information from the engineer when they wanted to prioritize locations or learn about threat rooms. When the GPS systems failed, more than half of the time players asked about threat rooms, not victims, rubble, or other events.

Figures 3 and 4 show that what is assumed to be trivial isn't always trivial. For example, figure 3 shows that given a player's intention to wake a critical, they are 55\% likely to ask another player to join them (trivial). It is far more interesting to see (figure 4) that given the question utterance 'requestDestination', it is more than 80\% likely to be asked only for the intent of waking a critical victim.





\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/QuestionTiming_STA.pdf}}
    \caption{Questions asked with respect to time. Time is represented continuously as color within the histogram. Video observations and question count are represented as a histogram. }
    \end{figure}
    
Figure 5 demonstrates questions asked across time. Team 218 (videos 635 and 636) asked the fewest questions and scored the lowest combined point. Video 636 is colored red in more than 66\% of its bars, meaning that more than 66\% of questions were asked toward the end of the mission. Compare this to team 1 (videos 633 and 634), who were relatively quiet players and asked the most questions in the beginning of each mission. Team 3 asked the most questions. The even distribution of red and black demonstrates how they asked questions at a \emph{consistent rate} throughout their missions.


\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{../images/QuestionUtterances_Link_STA.pdf}}
    \caption{Conditional probabilities of player intention, given the words that a player speaks when asking a question.}
    \end{figure}
    
\begin{figure}[h!]
    \centering
    \fbox{\includegraphics[width=0.8\textwidth]{"../images/Intent_to_Utterances_Link_STA.pdf"}}
    \caption{Conditional probabilities of words to be uttered in a question, given the player's intention.}
    \end{figure}
    Figure 6 shows how some words can infer trivial player intention (ie. 'information' utterances show 'shareInformationUnique' intention). However, words such as 'say', 'type', or 'come' are not domain-specific to the Minecraft environment-- yet show team collaboration about critical victims. 'Say' is the stemmed keyword that shows teammate communication and clarification (ie "Did you say ...") Compare this to words like 'critical' or 'area', which do not show team collaboration as much as it shows individual intention.  
    
    Figure 7 shows the probability of a word utterance, given the player intention. Returning to the intent of 'collaborateCriticalWake' from figure 7, the two most likely words to be uttered are 'want' and 'information.' From figure 6, only the words 'come', 'critical', and 'help' tie back to this intent. 






