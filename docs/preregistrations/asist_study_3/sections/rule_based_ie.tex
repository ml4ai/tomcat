\label{ch:rule_based_ie}
\textbf{Remo Nitschke}

\section{Introduction}

For humans, verbal and non-verbal communication are insightful indicators of
social processes. Within the scope of ASIST, team-building and communication
within teams is of heightened interest. We maintain, that in order to develop a
model of human agents and human teams, surveying verbal communication is
essential. Our DialogAgent is designed with this goal in mind: Extract
information from player dialog that can be useful in order to create a model of
the player's mental state.

An AI agent needs to formulate a model of the humans and teams it advises. In
order to construct such a model, language data provides essential insights. As
raw natural language data is ``messy", we believe it is necessary to
pre-process and vet this data into chunks that are informative and pertinent to
the AI. This is done via our DialogAgent which provides Dialog Act Labels and
Event Extractions for player utterances. The Dialog Act Labels inform on the
\emph{type} of utterance, whereas the Event Extractions inform on the semantic
\emph{content} of the utterance. In this section we discuss the engine that
constructs the Event Extractions.

\section{Approach}

%% Note to Adarsh: I think we could insert the diagram from Study 2 prereg here (page 2)

Raw communication audio captured during the experiments is processed through
our SpeechAnalyzer which provides transcriptions (transcriptions:
MinecraftEntity\_Observation\_Asr\_Speechanalyzer). The transcriptions are run through our DialogAgent which contains an extensive rule-based event extraction system (ODIN) \citep{valenzuela-escarcega-etal-2016-odins}. Much of modern information extraction and event extraction is done via neural nets, see \citet{Ahmad2021GATEGA,Du2020EventEB}. We are opting for a rule-based system because:

\begin{enumerate}
 \item It allows us to be more flexible with our extraction labels. We can quickly add or remove labels if we see the need to do so.
 \item It allows for high precision for the labels we are interested in. Rules can be crafted to be precise (albeit at the cost of coverage).
 \item Rules do not require us to create, maintain, and annotate extensive datasets. This is especially pertinent within the scope of ASIST, as domain specific terms can change between Studies. It would be near impossible for us to train a neural agent on the new terms of the next study before the data is published.
\end{enumerate}

Our rule based framework returns nested labels and their spans. The nesting of labels represents the argument structure of the event. The extracted events are returned as JSON objects (event extractions: MinecraftEntity\_Event\_Dialogue\_Event\_Dialogagent). 


\subsection{List of Variables}
\begin{itemize}
    \item Input Variables
    \begin{itemize}
        \item transcriptions: MinecraftEntity\_Observation\_Asr\_Speechanalyzer
        %the following only if we want idc in pre reg:
        %\item location data: MinecraftEntity_Event_Location_Locationmonitor
        % \item triage status: MinecraftEntity_Event_Triage_Simulator
        %\item rubble interaction: MinecraftEntity_Event_Rubbledestroyed_Simulator
    \end{itemize}
    \item Output Variables
    \begin{itemize}
        \item Event Extractions: MinecraftEntity\_Event\_Dialogue\_Event\_Dialogagent
    \end{itemize}
\end{itemize}


\section{Evaluation}
We will evaluate via manual evaluation by human annotators. We will hire human annotators to evaluate a representative chunk of utterances. As the DialogAgent currently extracts 100+ labels, we have narrow down this selection in order to avoid overwhelming our annotators. 
We will select 20 labels for annotation. These labels will be selected for two qualities:
\begin{enumerate}
\item Complexity: The DialogAgent labels can be divided in two types. Labels which are generated by string-pattern matching and labels which are generated by a combination of pattern matching, dependency parsing, and tagging. We will call the latter ``complex", as they involve reliance on a dependency parse and are more unpredictable due to the fact that they rely on multiple conditions to trigger. For this evaluation, ``complex" labels are more pertinent, as their accuracy is harder to predict. \footnote{A brief explanation as to why ``simple" labels are less interesting. Assume a label for players talking about ``victims". This label will be generated by a simple pattern matching rule that scans all tokens for a regular expression looking something like this: {/(?i){$\hat$ }victims?\$/}. This pattern will match any and all occurrences of ``victim" (plural or singular). The precision and recall of this label now depends entirely on the quality of the transcript and on the way the players talk about ``victims" (for example, they may use other terms as well). This makes this label much more predictable, than one that relies on dependency graphs in addition to pattern matching.}
\item High Frequency: The selected label should be sufficiently high in frequency to generate a representative amount of data. As our labels are often very specific, and thus rare, we will circumvent this issue by letting annotators annotate for groupings of labels.\footnote{For example: We have five different labels for players ``needing something", such as a specific item, a specific role, or just help from other players. In order to lighten the load on annotators, we will subsume these under one label.}
\end{enumerate}

Annotators will receive transcripts of player communications. They will be asked to annotate the transcripts for the 20 labels we have giventhem. With this annotated data, we can write a script that will counter-check the DialogAgent extractions against the annotations and calculate a representative F1 score.



\subsection{Precision Evaluation for the Entire Label-Set}
We will also run a seperate evaluation for precision,\footnote{For reasons of economy, we restrict this evaluation to precision. Our expert team-members can judge produced labels for precision at a much higher speed than they can annotate utterances for labels.} done by team members and hired annotators who are familiar with our DialogAgent labels.

We have done a preliminary evalution of this type already, and have found \ldots [NUMBERS HERE]

\subsection{Potential Evaluation Problems}
By subsuming multiple labels under one label for the general evaluation, we run the risk of having no way of discerning if one label in a group is a particularly weak performer. For example, assume we have five different labels for types of questions. Assume we subsume them under one generic ``Question" label for this evaluation. After the annotations are done, we calculate an F1 score for this new label. If one of the five labels was a weak performer (for example, .3 F1), but the others performed well, then the overall F1 score for this label will mask this fact. 

In order to combat this problem, we will run an evaluation for precision over the entire label set, as described in the previous subsection. Unfortunately, we will not be able to generate recall for the entire label set, as this process is simply too costly for 100+ labels.

