\chapter{Rule-based entity and event extraction}
\label{ch:rule_based_ie}
\textbf{Remo Nitschke}

\section{Introduction}

For humans, verbal and non-verbal communication are indicators that provide
significant insight into social processes. Within the scope of the ASIST
program, team-building and communication within teams is of heightened
interest. We assert that understanding the semantics of verbal communication is
essential to develop a model of human individuals and teams. Our `DialogAgent'
AC is designed with the goal of extracting information from player dialog that
can help build and update a model of players' mental states. The outputs of the
DialogAgent AC are consumed by the ToMCAT ASI (\autoref{ch:pgm}).

Raw natural language data is messy by nature - in order for a machine to reason
about its semantics, it must be processed to extract structured information.
In order to do this, the DialogAgent leverages Odin
\cite{valenzuela-escarcega-etal-2016-odins}, a powerful rule-based information
extraction system developed at UArizona.  While much of modern information
extraction and event extraction is done using neural networks
\cite{Ahmad2021GATEGA, Du2020EventEB}, we opt for a rule-based system for the
following reasons:

\begin{enumerate}

 \item It allows us to be more flexible with our extraction labels. We can
     quickly add or remove labels if we see the need to do so.

 \item It allows for high precision for the labels we are interested in. Rules
     can be crafted to be precise (albeit at the cost of recall).

 \item Rules do not require us to create, maintain, and annotate extensive
     datasets. This is especially pertinent to the ASIST program, with the
     relatively fast cadence of changes to the experimental task and domain,
     along with the concomitant changes to domain-specific vocabulary of
     interest.  It would be near-impossible for us to annotate data and train a
     neural agent on the new vocabulary of a study prior to its actual
     execution and the release of the data from it. 

\end{enumerate}

\section{Hypotheses}

We will test the following hypothesis: 

- Our rule-based event extraction system will be able to achieve a minimum
precision of 91\% for simple labels and 75\% for complex labels, as defined in
\autoref{subsec:rule_evaluation}

\section{Approach}

The DialogAgent processes natural language texts from two sources: utterances
spoken by players, and text chats sent using the Minecraft chat
interface\footnote{For ASIST Study 3, participants are discouraged from using
    text chat, but it is not programmatically disallowed - for this reason, we
    still monitor it. Performer teams who use the outputs of the DialogAgent
    should inspect the \texttt{data.participant\_id} key in the DialogAgent
    output messages to make sure that it is not \texttt{Server}, which
correspnods to chat messages sent by the testbed infrastructure rather than
humans.}.

Our rule-based framework extracts \emph{events} from natural language. These
can be either \emph{simple} events, that do not have any arguments, or
\emph{complex} events, that can take other events as arguments. Each event is
associated with a unique span of text and is assigned one or more labels by the
rule that extracts it. The labels we are using are organized into an ontology.
If a rule assigns a label to an event from this ontology, the parents of that
label in the ontology are also assigned as labels for that event.

\section{Evaluation}

We will evaluate via manual evaluation by human annotators. We will hire human
annotators to evaluate a representative chunk of utterances. As the DialogAgent
currently extracts 100+ labels, we have narrow down this selection in order to
avoid overwhelming our annotators.  We will select 20 labels for annotation.
These labels will be selected for two qualities:

\begin{enumerate}

\item Complexity: The DialogAgent labels can be divided in two types. Labels
    which are generated by string-pattern matching and labels which are
    generated by a combination of pattern matching, dependency parsing, and
    tagging. We will call the latter ``complex", as they involve reliance on a
    dependency parse and are more unpredictable due to the fact that they rely
    on multiple conditions to trigger. For this evaluation, ``complex" labels
    are more pertinent, as their accuracy is harder to predict. \footnote{A
        brief explanation as to why ``simple" labels are less interesting.
        Assume a label for players talking about ``victims". This label will be
        generated by a simple pattern matching rule that scans all tokens for a
        regular expression looking something like this: 
               \texttt{/(?i){\textsuperscript{$\wedge$} }victims?\$/}
        .  This pattern will match any and all occurrences of
        ``victim" (plural or singular). The precision and recall of this label
        now depends entirely on the quality of the transcript and on the way
        the players talk about ``victims" (for example, they may use other
        terms as well). This makes this label much more predictable, than one
        that relies on dependency graphs in addition to pattern matching.}

\item High Frequency: The selected label should be sufficiently high in
    frequency to generate a representative amount of data. As our labels are
    often very specific, and thus rare, we will circumvent this issue by
    letting annotators annotate for groupings of labels.\footnote{For example:
    We have five different labels for players ``needing something", such as a
specific item, a specific role, or just help from other players. In order to
lighten the load on annotators, we will subsume these under one label.}

\end{enumerate}

Annotators will receive transcripts of player communications. They will be
asked to annotate the transcripts for the 20 labels we have given them. With
this annotated data, we can write a script that will counter-check the
DialogAgent extractions against the annotations and calculate a representative
F1 score.



\subsection{Precision Evaluation for the Entire Label-Set}

We will also run a separate evaluation for precision\footnote{For reasons of
economy, we restrict this evaluation to precision. Our expert team-members can
judge produced labels for precision at a much higher speed than they can
annotate utterances for labels.}, done by team members and hired annotators who
are familiar with our DialogAgent labels.


\subsubsection{Preliminary Precision Evaluation} 
\label{subsec:rule_evaluation}

We have done a preliminary evalution of 1700 labels from the pilot data for
precision already, and have found that the ``simple" labels are highly precise.
``Simple" labels. In our preliminary evalution, simple labels yielded between
91-100\% precision.\footnote{This is for labels that had more than 20 entries
in our evaluation} ``Complex" labels are found to yield between 75-94\%
precision.\footnote{This is for labels that had more than 10 entries in our
evaluation}

\subsection{Potential Evaluation Problems}

By subsuming multiple labels under one label for the general evaluation, we run
the risk of having no way of discerning if one label in a group is a
particularly weak performer. For example, assume we have five different labels
for types of questions. Assume we subsume them under one generic ``Question"
label for this evaluation. After the annotations are done, we calculate an F1
score for this new label. If one of the five labels was a weak performer (for
example, .3 F1), but the others performed well, then the overall F1 score for
this label will mask this fact. 

In order to combat this problem, we will run an evaluation for precision over
the entire label set, as described in the previous subsection. Unfortunately,
we will not be able to generate recall for the entire label set, as this
process is simply too costly for 100+ labels.
