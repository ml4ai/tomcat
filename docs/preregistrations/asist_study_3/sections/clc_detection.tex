\chapter{Closed-loop communication detection}
\label{ch:clc}
\textbf{Yuwei Wang}

\section{Introduction}

Good teamwork and communication can enable a team of agents (human or
otherwise) to perform beyond the sum of their parts \citep{roberts2022state}.
In particular, during high-stakes crisis situations, miscommunication is a
common cause of adverse events
\citep{taylor2014description,davis2017operative}. Closed-loop communication
(CLC) is often recommended in the team research literature as a communication
behavior that can guarantee the accuracy of information exchange
\citep{marzuki2019closed}. Currently, CLC in spoken dialogue is identified via
retrospective analyses involving manual transcription and annotation. However,
given the potentially catastrophic consequences of poor team communication
\citep{flin2004identifying} - especially in complex, fast-paced, and
high-stakes environments such as urban search and rescue scenarios - we argue
that there is an urgent need for AI technologies that can detect and repair the
breakdowns in CLC as they happen. To address this need, we propose to develop
an AI system for detecting the presence or absence of CLC procedures in spoken
dialogue within teams of humans collaborating on shared tasks.

Currently, most real-time natural language understanding (NLU) and responding
systems are limited to conversing with a single human at a time. On the other
hand, there are numerous analyses of multi-participant spoken dialogue in the
academic literature - however, these are primarily performed offline rather
than in real-time, and the communicative events in their multi-party
conversations are manually coded rather than automatically extracted using
information extraction (IE) methods (e.g., \citep{jagannath2022speech}).

The ASR Agent in conjunction with the DialogAgent (see \autoref{ch:rule_based_ie})
enables us to extract events of interest in real time from spoken dialogue,
which goes a long way towards addressing both of these limitations. However,
the DialogAgent at the level of individual utterances, and does not reason
about multi-utterance context. For this reason, we are developing a separate downstream
CLC detection component that utilizes the outputs of the DialogAgent, but also
reasons about context and state more deeply.

We will implement a set of CLC detection rules that reason about the events
extracted by the DialogAgent as well as the identity of the speakers
corresponding to the utterances.


\section{Approach}

While there are different definitions of closed-loop communication in the
literature (\citet{abd2018closed}, \citet{yee2017role}), for our module we
adopt the definition proposed by \citet{Hargestam.ea:2013}.  In this definition
of CLC, there are three distinct phases (see \autoref{tab:clc-three-phases}): 

\begin{itemize}

    \item \textbf{Call-out}: The sender initiates a message.

    \item \textbf{Check-back}: The receiver acknowledges the message, usually
        by paraphrasing or repeating the main information of the message.

    \item \textbf{Closed-loop}: The sender verifies that the message has been
                received and interpreted correctly.

\end{itemize}

\newcommand{\utteranceone}{\textit{This is Green. I’m finishing this side, blue, could you check the central for victims? }} 
\newcommand{\utterancetwo}{\textit{This is Blue. Okay. I’ll go check the central for victims.}}
\newcommand{\utterancethree}{\textit{All right, thank you, Blue.}}

\begin{table}
    \footnotesize
    \centering
    \begin{tabularx}{6in}{lXlp{1.2in}p{1in}p{0.4in}}
        \toprule
        Role  & Utterance         & Phase       & Trigger Rules: mention text  & Argument rules: mention text                                                         & CLC Label\\\midrule
        Green & \utteranceone{}   & Call-out    & Instruction: could you check & Action: check \newline Location: central \newline Victim: victims                    & 1a\\\\
        Blue  & \utterancetwo{}   & Check-back  & Agreement: okay              & Action: check \newline Location: central \newline Victim: victims \newline Blue:blue & 1b\\\\
        Green & \utterancethree{} & Closed-loop & Agreement: All right         &                                                                                      & 1c\\
        \bottomrule
    \end{tabularx}
    \caption{%
        An example of the three phases of closed-loop communication. This
        example is extracted from Study 2 data, while Green is the sender of the
        request message, and Blue is the receiver of the request message.
    }
    \label{tab:clc-three-phases}
\end{table}

We are going to develop a procedure to dynamically detect the three phases of
CLC. First, the Call-out phase will be detected by rules in the DialogAgent.
Labels including HelpRequest, Instruction, and NeedAction are used as
triggers for this event. Next, we examine the a window of the subsequent five
utterances in the dialogue.

If we find acknowledgement by other team members as well as the same
semantic labels as in the Call-out message, then this indicates there is a
Check-back phase. Finally, within five utterances following the Check-back, if
we see the sender verified the information of the Check-back with the Agreement
label, we can determine that this is a closed-loop communication. The triggers
and argument rules of CLC detection are illustrated in
in Algorithm 1.


%\begin{table*}[tb]
    %\centering
    %\begin{tabular}{lp{1in}lll}
        %\toprule
        %Role  & Utterance         & Trigger Rules: mentioned text & Argument Rules: mentioned text                                                       & CLC label \\\midrule
        %Green & \utteranceone{}   & Instruction: could you check  & Action: check \newline Location: central \newline Victim: victims                    & 1a\\
        %Blue  & \utterancetwo{}   & Agreement: okay               & Action: check \newline Location: central \newline Victim: victims \newline Blue:blue & 1b\\
        %Green & \utterancethree{} & Agreement: All right          &                                                                                      & 1c\\
        %\bottomrule
    %\end{tabular}
    %\caption{The trigger and argument rules of CLC detection}
    %\label{tab:clc-triggers-labels}
%\end{table*}

\subsection{List of Variables}

\begin{itemize}
    \item Input Variables
    \begin{itemize}
        \item transcriptions: MinecraftEntity\_Observation\_Asr\_Speechanalyzer
    \end{itemize}
    \item Output Variables
    \begin{itemize}
        \item Event Extractions: MinecraftEntity\_Event\_Dialogue\_Event\_Dialogagent
    \end{itemize}
\end{itemize}

\begin{algorithm}
    \caption{The algorithm for CLC detection}\label{alg:CLC}
    \begin{algorithmic}[1]
        \State $CLC-labels \gets []$
        \Function{CallOut}{utterance} 
            \If{any(label in [‘HelpRequest’, ‘NeedAction’, ‘Instruction’] for label in $utterance$}
            \State $CLC-labels \gets 1a$
            \EndIf
            \EndFunction

        \Function{CheckBack}{utterance} in range (5)

            \If{'Acknowledgement' in labels of next $utterance$
            \If{any(label in CallOut.arguments for label in next $utterance$}
                \State $CLC-labels \gets 1b$
                \EndIf
                } else
            \State $CLC-labels \gets 0.5b$
            \EndIf
            \EndFunction
        \Function{CloseLoop}{utterance} in range (5)
            \If{'Acknowledgement' in labels of next $utterance$}
            \State $CLC-labels \gets 1c$
            \EndIf
            \EndFunction
    \end{algorithmic}
\end{algorithm}

\section{Evaluation}

The performance of the closed-loop communication detector will be evaluated by
human annotators. Annotators will be trained to evaluate the automatically
extracted CLC dialogues with the CLC coding scheme in Table 2. For a weak
check-back that only acknowledge the call-out with the ‘Agreement’ label but no
repeating information is found, the check-back should be labeled as ‘0.5b’
instead of the complete check-back label ‘1b’.  The inter-rater reliability of
the annotators will be measured using Cohen’s kappa. When the percentage of
agreement between annotators reaches 80\%, and K>.70, annotators could start
work on the formal annotation of the data. The precision and F1 score will be
used to evaluate the performance of our CLC detector. We will improve the
detecting rules and algorithms according to the evaluation. The precision
should reach a minimum of 90\% and F1 reach a minimum of 80\% after improvement
on the CLC detector has been fully applied.
