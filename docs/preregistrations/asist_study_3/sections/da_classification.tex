\chapter{Dialogue act classification}
\textbf{Ruihong Huang, Ayesha Qamar, Messal}

\section{Introduction}

In the Theory of Mind-based Cognitive Architecture for Teams (ToMCAT) project,
we are working on building an AI agent that will assist the players by taking
their speech and facial expressions as input. Speech being a core part of the
inputs, the agents will need to be capable of natural language understanding to
process the conversations to make informed and prompt decisions. Identifying
Dialog Acts (DA) is one of the primary aspects of natural language
understanding. dialog Act (DA) can be identified as a method of defining the
semantic content and communicative function of a single utterance of dialog
\citep{Searle:1969}. Examples include request, question, acknowledgment etc.
Dialog acts can provide important information about the user dialog turns and
set of possible system actions, and the frequencies and patterns of DAs spoken
by different speakers could also potentially indicate the roles they play such
as leader, follower etc., Thus, it is a useful capability for this AI agent and
many conversational agents in general.

Acknowledging the importance of DA classification in natural language
understanding, extensive research has been conducted on DA classification.
These research works have taken different approaches in terms of dataset,
machine learning model and input to the models. The most popular datasets that
are being used are Switchboard Corpus (SwDA) and ICSI Meeting Recorder Dialog
Act Corpus (MRDA). Most of the approaches use textual utterances from dialog
transcripts as input to the model. Statistical machine learning models such as
Support Vector Machines (SVMs) \citep{Henderson.ea:2012}, Hidden Markov Models
(HMMs) \citep{Stolcke.ea:2000}, Conditional Random Fields (CRFs)
\citep{Zimmermann:2009} are employed for identifying DAs. Deep learning models
are also gaining popularity in DA classification. \citet{Liu.ea:2017} presented
both CNN models and hierarchical CNN+CNN and CNN+RNN models to classify dialog
acts and showed that RNN/Bi-LSTM on top of CNN model performs better than other
models in consideration. \citet{Shen.ea:2016} presented that Neural Attention
Model with context information performed well on the SwDA dataset.
\citet{Raheja.ea:2019} achieved state-of-the-art result using context-aware
self-attention model on MRDA corpus. Another approach for DA classification is
incorporating both lexical and acoustic features. \citet{Ortega.ea:2018} showed
that their Lexico-Acoustic neural network models can outperform the similar
models taking only lexical information as input.

Even though these approaches provide excellent results on DA classification,
they lack in various aspects. First of all, most of these models require clean
transcripts as input. Achieving clean transcripts requires manual annotation,
which is both time consuming and costly. As a result, DA classification in real
time is not possible. Another limitation is the lack of explicit addressing to
multiparty dialogs where mechanisms to incorporate speaker identification and
determining the discourse structure is also crucial. Apart from these
limitations, these approaches often use only 5 types of high level tags for DA
classification, which do not entirely explain the DA under question. It is
necessary to have both general and specific tags to completely understand a DA.

We are going to develop a Deep neural network based DA classifier to process
the input utterances in real time which is more aligned with the main goal of
the project that is building an AI agent to be an effective teammate of a
Minecraft player. As the players will play online simultaneously, the agent
needs to understand the conversations of the player as the game progresses. For
the agent to be real time, we cannot rely on manual transcription. Instead we
have used a publicly available Automatic Speech Recognizer (ASR), named Google
Cloud Speech API to convert the utterances into text. This provides noisy text
in return which might harm the performance of the DA classifier. To compensate
for this, we will use acoustic features from the raw speech as well.

A Bi-LSTM based baseline model is already trained with clean transcripts. The
same model is trained again with the ASR generated transcripts and the
performance dropped significantly due to ASR noise and the highly overlapping
nature of the utterances in the meetings. To overcome this drop we will use a
fusion based audio-language model to leverage both lexical and acoustic
information of the utterances to successfully identify the DAs. In addition,
our approach will capture the threading structure within a dialogue that
involves detecting utterances falling within an adjacency pair (consisting of a
question and an answer utterance, or a request and an acceptance utterance) and
then linking them together. We will aim to eventually jointly learn both the
threading structure and DAs of a dialog in a multitask learning setting since
we expect the two tasks to benefit each other.

Currently we are using a multi-party dialog dataset MRDA
\citep{Shriberg.ea:2004} that consists of 75 meetings each about an hour long,
where each utterance has one (out of 11) general and zero or more (out of 40)
specific tags. Once the experimentation is done, we will use a transfer
learning approach to train and test the model for study-2 data. 

\section{Evaluation}

To evaluate our approach, we will use F1 score as the evaluation metric. For
multiclass classification problems, especially where the classes are highly
imbalanced, F1 score provides more insight than accuracy.  We also intend to
annotate ASIST data for dialog Acts so that the system trained on MRDA can be
finetuned on ASIST.
