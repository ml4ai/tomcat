\chapter{Dialogue act classification}
\label{ch:da_classification}
\textbf{Ruihong Huang, Ayesha Qamar, Md Messal Monem Miah}

\section{Introduction}

In the Theory of Mind-based Cognitive Architecture for Teams (ToMCAT) project,
we are working on building an AI agent that will assist the players by taking
their speech and facial expressions as input. Speech being a core part of the
inputs, the agents will need to be capable of natural language understanding to
process the conversations to make informed and prompt decisions. Identifying
Dialog Acts (DA) is one of the primary aspects of natural language
understanding. dialog Act (DA) can be identified as a method of defining the
semantic content and communicative function of a single utterance of dialog
\citep{Searle:1969}. Examples include request, question, acknowledgment etc.
Dialog acts can provide important information about the user dialog turns and
set of possible system actions, and the frequencies and patterns of DAs spoken
by different speakers could also potentially indicate the roles they play such
as leader, follower etc., Thus, it is a useful capability for this AI agent and
many conversational agents in general.

Acknowledging the importance of DA classification in natural language
understanding, extensive research has been conducted on DA classification.
These research works have taken different approaches in terms of dataset,
machine learning model and input to the models. The most popular datasets that
are being used are Switchboard Corpus (SwDA) and ICSI Meeting Recorder Dialog
Act Corpus (MRDA). Most of the approaches use textual utterances from dialog
transcripts as input to the model. Statistical machine learning models such as
Support Vector Machines (SVMs) \citep{Henderson.ea:2012}, Hidden Markov Models
(HMMs) \citep{Stolcke.ea:2000}, Conditional Random Fields (CRFs)
\citep{Zimmermann:2009} are employed for identifying DAs. Deep learning models
are also gaining popularity in DA classification. \citet{Liu.ea:2017} presented
both CNN models and hierarchical CNN+CNN and CNN+RNN models to classify dialog
acts and showed that RNN/Bi-LSTM on top of CNN model performs better than other
models in consideration. \citet{Shen.ea:2016} presented that Neural Attention
Model with context information performed well on the SwDA dataset.
\citet{Raheja.ea:2019} achieved state-of-the-art result using context-aware
self-attention model on MRDA corpus. Another approach for DA classification is
incorporating both lexical and acoustic features. \citet{Ortega.ea:2018} showed
that their Lexico-Acoustic neural network models can outperform the similar
models taking only lexical information as input.

Even though these approaches provide excellent results on DA classification,
they lack in various aspects. First of all, most of these models require clean
transcripts as input. Achieving clean transcripts requires manual annotation,
which is both time consuming and costly. As a result, DA classification in real
time is not possible. Another limitation is the lack of explicit addressing to
multiparty dialogs where mechanisms to incorporate speaker identification and
determining the discourse structure is also crucial. Apart from these
limitations, these approaches often use only 5 types of high level tags for DA
classification, which do not entirely explain the DA under question. It is
necessary to have both general and specific tags to completely understand a DA.

We are going to develop a Deep neural network based DA classifier to process
the input utterances in real time which is more aligned with the main goal of
the project that is building an AI agent to be an effective teammate of a
Minecraft player. As the players will play online simultaneously, the agent
needs to understand the conversations of the player as the game progresses. For
the agent to be real time, we cannot rely on manual transcription. Instead we
have used a publicly available Automatic Speech Recognizer (ASR), named Google
Cloud Speech API to convert the utterances into text. This provides noisy text
in return which might harm the performance of the DA classifier. To compensate
for this, we will use acoustic features from the raw speech as well.

\section{Approach}
A Bi-LSTM based baseline model is already trained with clean transcripts. The
same model is trained again with the ASR generated transcripts and the
performance dropped significantly due to ASR noise and the highly overlapping
nature of the utterances in the meetings. To overcome this drop we will use a
fusion based audio-language model to leverage both lexical and acoustic
information of the utterances to successfully identify the DAs. In addition,
our approach will capture the threading structure within a dialogue that
involves detecting utterances falling within an adjacency pair (consisting of a
question and an answer utterance, or a request and an acceptance utterance) and
then linking them together. We will aim to eventually jointly learn both the
threading structure and DAs of a dialog in a multitask learning setting since
we expect the two tasks to benefit each other.

Currently we are using a multi-party dialog dataset MRDA
\citep{Shriberg.ea:2004} that consists of 75 meetings each about an hour long,
where each utterance has one (out of 11) general and zero or more (out of 40)
specific tags. Once the experimentation is done, we will use a transfer
learning approach to train and test the model for study-2 data. 

MRDA data is a multiparty multilabel dialog act corpus. So, each utterance in
the dataset may have multiple tags associated with it which poses a problem for
building an efficient model that could correctly capture the dialog acts. One approach
to solve this multi-label problem is randomly choosing a label from 
the label combinations. However, this approach often does not reflect the most
prominent purpose of each utterance. So, we have drawn inspiration from label
disambiguation from related corpus and carefully looked at examples to come up
with a set of precedence rules for the labels and thus solved the multi-label
challenge. This approach significantly improves the performance of our baseline
model on MRDA data.

\section{Evaluation}

To evaluate our approach, we will use F1 score as the evaluation metric. For
multiclass classification problems, especially where the classes are highly
imbalanced, F1 score provides more insight than accuracy.  We also intend to
annotate ASIST data for dialog Acts so that the system trained on MRDA can be
fine-tuned on ASIST. The baseline model that we are currently using is a sequential
model consisting of a hierarchical LSTM. The macro F1 score for this model on MRDA
data is 40.77. However, when we evaluate the trained model on ASIST study 3 data, 
the accuracy and F1 score both degrade. The performance of the model on raw ASIST
data in terms of accuracy and macro F1 score is 30\% and 9.705 respectively. 
One of the key factors behind this performance degradation is noisy transcripts 
generated by the ASR. In order to solve this issue we have to manually correct the
transcriptions generated by the ASR. After correcting the transcripts, we have
observed a significant improvement of the model performance both in terms of accuracy
and F1 score. The accuracy and macro F1 score for the cleaned ASIST study 3 data are 
37\% and 18.86 respectively. Highlights of the results on some important classes are 
shown in the tables below. The first table is contains the result for noisy transcripts 
and the second table contains the results for corrected transcripts.

\begin{center}
\begin{tabular}{||c c c c||}
 \hline
 Label & Precision & Recall & F1 Score\\ [0.5ex]
 \hline\hline
 Yes/No Question & 0.80 & 0.39  & 0.52\\
 \hline
 Statement & 0.36 & 0.73 & 0.48\\
 \hline
 Command \& Suggestion & 0.52 & 0.46 & 0.49\\
 \hline
 Commitment & 0.33 & 0.01 & 0.01\\
 \hline
 Accept & 0.45 & 0.40 &  0.42 \\
 \hline
  Reject & 0.38 & 0.33 & 0.35\\
 \hline
  Understanding Check & 0.18 & 0.34 & 0.24\\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c c c c||}
 \hline
 Label & Precision & Recall & F1 Score\\ [0.5ex]
 \hline\hline
 Yes/No Question & 0.50 & 0.01 & 0.01\\
 \hline
 Statement & 0.32 & 0.73 & 0.44\\
 \hline
 Command \& Suggestion & 0.47 & 0.47 & 0.47\\
 \hline
 Commitment & 1.00 & 0.01 & 0.01\\
 \hline
 Accept & 0.39 & 0.31 &  0.34 \\
 \hline
  Reject & 0.50 & 0.35 & 0.41\\
 \hline
  Understanding Check & 0.18 & 0.34 & 0.24\\
 \hline
\end{tabular}
\end{center}

We can observe significant improvement for almost all of the interesting tags when the
utterances are corrected manually. Especially for the yes-no question class we can 
observe massive jumps in both precision and recall. However, some classes like commitment
are suffering from low recall performance even for the corrected transcripts. Moving forward
our next step will be to annotate enough data to fine-tune the model on ASIST to achieve
even better performance.

