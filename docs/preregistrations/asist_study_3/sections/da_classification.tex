\chapter{Dialogue act classification}
\label{ch:da_classification}
\textbf{Ruihong Huang, Ayesha Qamar, Md Messal Monem Miah, Adarsh Pyarelal}

\section{Introduction}

Spoken natural language is the primary medium of communicating and coordinating
within the 3-person teams that will be participating in ASIST Study 3.  Given
this, AI agents will need to be capable of natural language understanding to
process the conversations to make informed and prompt decisions. Identifying
dialog acts (DAs) is one of the primary aspects of natural language
understanding. A dialog act can be identified as a method of defining the
semantic content and communicative function of a single utterance of dialog
\citep{Searle:1969}. Examples of dialog acts include `request', `question',
`acknowledgment', etc.
Dialog acts can provide important information about the user dialog turns and
set of possible system actions. The patterns of dialog acts
of different speakers could also potentially indicate the roles they play in a
team, e.g., leader, follower, etc., Thus, it is a necessary capability for our
ToMCAT architecture as well as for conversational agents in general.


Extensive research has been conducted on dialog act classification due to its
importance in natural language understanding. These research works have taken
different approaches in terms of datasets, machine learning models and input to
the models. The most widely used datasets are the Switchboard
Corpus (SwDA) and ICSI Meeting Recorder Dialog Act Corpus (MRDA)
\citep{Shriberg.ea:2004}. Most of the approaches use textual utterances from
dialog transcripts as input to their models. Initial efforts to identify DAs
used classic statistical machine learning models
such as support vector machines (SVMs) \citep{Henderson.ea:2012}, hidden Markov
models (HMMs) \citep{Stolcke.ea:2000}, and conditional random fields (CRFs)
\citep{Zimmermann:2009}.

More recently, deep learning models have been gaining in popularity for DA
classification. \citet{Liu.ea:2017} presented both CNN models and hierarchical
CNN+CNN\footnote{CNN: Convolutional neural network.} and CNN+RNN\footnote{RNN:
Recurrent neural network} models to classify dialog acts and showed that a
RNN/Bi-LSTM\footnote{LSTM: Long short-term memory (a type of neural network
architecture).} on top of a CNN model performs better than the other models
they considered.  \citet{Shen.ea:2016} showed that a neural attention model
with context information performed well on the SwDA dataset.
\citet{Raheja.ea:2019} achieved state-of-the-art results using a context-aware
self-attention model on the MRDA corpus. Another approach for DA classification
involves incorporating both lexical and acoustic features. This approach is
motivated by the fact that humans use vocalic features to express specific
dialog acts - for example, people often raise their pitch slightly at the end
of a question. \citet{Ortega.ea:2018} showed that their lexico-acoustic neural
network models outperformed similar models that took only lexical information
as input.

While these approaches provide excellent results on DA classification, they are
lacking in some important aspects.
First, most of these models require clean
transcripts as input. Achieving clean transcripts requires manual annotation,
which is both time consuming and costly. As a result, these models cannot be
used for DA classification in real time where the only transcripts available
are imperfect ones produced by automatic speech recognition (ASR) systems.
Second, the approaches do not explicitly address dialogs where mechanisms to
incorporate speaker identification and determining the discourse structure are
also crucial.
Finally, the aforementioned approaches often use only 5 types of high-level
tags for DA classification, which frequently do not entirely explain the
purpose of the DA under consideration. It is necessary to have both general
(high-level) and specific tags to truly understand a DA.

To address these limitations, we will design and implement a deep neural
network based DA classifier to process input utterances in real time. This will
allow downstream AI agents to use dialog act information in order to provide
timely interventions. We will test this capability offline on Study 3 data and
deploy it online during Study 4 data collection.

needs to understand the conversations of the player as the game progresses. For
the agent to be real time, we cannot rely on manual transcription. Instead we
have used a publicly available Automatic Speech Recognizer (ASR), named Google
Cloud Speech API to convert the utterances into text. This provides noisy text
in return which might harm the performance of the DA classifier. To compensate
for this, we will use acoustic features from the raw speech as well.

\section{Approach}
A Bi-LSTM based baseline model is already trained with clean transcripts. The
same model is trained again with the ASR generated transcripts and the
performance dropped significantly due to ASR noise and the highly overlapping
nature of the utterances in the meetings. To overcome this drop we will use a
fusion based audio-language model to leverage both lexical and acoustic
information of the utterances to successfully identify the DAs. In addition,
our approach will capture the threading structure within a dialogue that
involves detecting utterances falling within an adjacency pair (consisting of a
question and an answer utterance, or a request and an acceptance utterance) and
then linking them together. We will aim to eventually jointly learn both the
threading structure and DAs of a dialog in a multitask learning setting since
we expect the two tasks to benefit each other.

Currently we are using a multi-party dialog dataset MRDA
 that consists of 75 meetings each about an hour long,
where each utterance has one (out of 11) general and zero or more (out of 40)
specific tags. Once the experimentation is done, we will use a transfer
learning approach to train and test the model for study-2 data. 

MRDA data is a multiparty multilabel dialog act corpus. So, each utterance in
the dataset may have multiple tags associated with it which poses a problem for
building an efficient model that could correctly capture the dialog acts. One approach
to solve this multi-label problem is randomly choosing a label from 
the label combinations. However, this approach often does not reflect the most
prominent purpose of each utterance. So, we have drawn inspiration from label
disambiguation from related corpus and carefully looked at examples to come up
with a set of precedence rules for the labels and thus solved the multi-label
challenge. This approach significantly improves the performance of our baseline
model on MRDA data.

\section{Evaluation}

To evaluate our approach, we will use F1 score as the evaluation metric. For
multiclass classification problems, especially where the classes are highly
imbalanced, F1 score provides more insight than accuracy.  We also intend to
annotate ASIST data for dialog Acts so that the system trained on MRDA can be
fine-tuned on ASIST. The baseline model that we are currently using is a sequential
model consisting of a hierarchical LSTM. The macro F1 score for this model on MRDA
data is 40.77. However, when we evaluate the trained model on ASIST study 3 data, 
the accuracy and F1 score both degrade. The performance of the model on raw ASIST
data in terms of accuracy and macro F1 score is 30\% and 9.705 respectively. 
One of the key factors behind this performance degradation is noisy transcripts 
generated by the ASR. In order to solve this issue we have to manually correct the
transcriptions generated by the ASR. After correcting the transcripts, we have
observed a significant improvement of the model performance both in terms of accuracy
and F1 score. The accuracy and macro F1 score for the cleaned ASIST study 3 data are 
37\% and 18.86 respectively. Highlights of the results on some important classes are 
shown in the tables below. The first table is contains the result for noisy transcripts 
and the second table contains the results for corrected transcripts.

\begin{center}
\begin{tabular}{||c c c c||}
 \hline
 Label & Precision & Recall & F1 Score\\ [0.5ex]
 \hline\hline
 Yes/No Question & 0.80 & 0.39  & 0.52\\
 \hline
 Statement & 0.36 & 0.73 & 0.48\\
 \hline
 Command \& Suggestion & 0.52 & 0.46 & 0.49\\
 \hline
 Commitment & 0.33 & 0.01 & 0.01\\
 \hline
 Accept & 0.45 & 0.40 &  0.42 \\
 \hline
  Reject & 0.38 & 0.33 & 0.35\\
 \hline
  Understanding Check & 0.18 & 0.34 & 0.24\\
 \hline
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{||c c c c||}
 \hline
 Label & Precision & Recall & F1 Score\\ [0.5ex]
 \hline\hline
 Yes/No Question & 0.50 & 0.01 & 0.01\\
 \hline
 Statement & 0.32 & 0.73 & 0.44\\
 \hline
 Command \& Suggestion & 0.47 & 0.47 & 0.47\\
 \hline
 Commitment & 1.00 & 0.01 & 0.01\\
 \hline
 Accept & 0.39 & 0.31 &  0.34 \\
 \hline
  Reject & 0.50 & 0.35 & 0.41\\
 \hline
  Understanding Check & 0.18 & 0.34 & 0.24\\
 \hline
\end{tabular}
\end{center}

We can observe significant improvement for almost all of the interesting tags when the
utterances are corrected manually. Especially for the yes-no question class we can 
observe massive jumps in both precision and recall. However, some classes like commitment
are suffering from low recall performance even for the corrected transcripts. Moving forward
our next step will be to annotate enough data to fine-tune the model on ASIST to achieve
even better performance.
